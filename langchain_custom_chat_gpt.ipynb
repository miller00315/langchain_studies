{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light from the Sun takes approximately 8 minutes and 20 seconds to travel to Earth. This is the time it takes for light to cover the 93 million miles (150 million kilometers) of distance between the Sun and Earth.\n",
      "\n",
      "--------------------------------------------------\n",
      "1. Light speed in vacuum is ...\n",
      "2. (No question provided)\n",
      "3. What about light speed in water\n",
      "4. how long does it take to ravel from sun to earth?\n",
      "\n",
      "--------------------------------------------------\n",
      "Goodbye\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "history = FileChatMessageHistory('chat_history.json')\n",
    "\n",
    "# memory object\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    chat_memory=history,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=['content'],\n",
    "    messages=[\n",
    "        SystemMessage(content='You are a chat bot having a converstaion with a human.'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'), # where the memry will be stored\n",
    "        HumanMessagePromptTemplate.from_template('{content}')\n",
    "    ] \n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "while True:\n",
    "    content = input('Your prompt')\n",
    "\n",
    "    if content.lower() in ['quit', 'exit', 'bye']:\n",
    "        print('Goodbye')\n",
    "        break\n",
    "\n",
    "    response = chain.run({'content': content})\n",
    "    print(response)\n",
    "    print()\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_chain_studies-WWOOpBwl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
