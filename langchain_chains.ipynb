{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miller/.local/share/virtualenvs/lang_chain_studies-WWOOpBwl/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a experience virologist.\n",
      "Write a few sentences about the following virus HIV in spanish.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'virus': 'HIV', 'language': 'spanish', 'text': 'El VIH, o virus de la inmunodeficiencia humana, es un virus que ataca al sistema inmunológico del cuerpo humano, específicamente a los linfocitos T CD4. Esto debilita la capacidad del cuerpo para combatir infecciones y enfermedades, lo que puede llevar al desarrollo del SIDA. El VIH se transmite principalmente a través de relaciones sexuales sin protección, el uso compartido de agujas contaminadas o de madre a hijo durante el embarazo, parto o lactancia. Actualmente, no existe una cura para el VIH, pero se puede controlar con medicamentos antirretrovirales para mantener a raya la replicación del virus y prevenir la progresión a SIDA.'}\n"
     ]
    }
   ],
   "source": [
    "from re import template\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "template = '''You are a experience virologist.\n",
    "Write a few sentences about the following virus {virus} in {language}.'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output = chain.invoke({'virus': 'HIV', 'language': 'spanish'})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhats is the capital of Italy? Lits teh top 3 places to visit in that city. Use bullet points.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The capital of Italy is Rome.\n",
      "\n",
      "- The Colosseum\n",
      "- The Vatican City\n",
      "- Trevi Fountain\n"
     ]
    }
   ],
   "source": [
    "template = 'Whats is the capital of {country}? Lits teh top 3 places to visit in that city. Use bullet points.'\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "country = input('Enter country: ')\n",
    "\n",
    "output = chain.invoke(country)\n",
    "\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSure! Here is a simple implementation of linear regression in Python:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def linear_regression(X, y):\n",
      "    # Add a column of ones to X for the intercept term\n",
      "    X = np.c_[np.ones(X.shape[0]), X]\n",
      "    \n",
      "    # Calculate the coefficients using the normal equation\n",
      "    theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
      "    \n",
      "    return theta\n",
      "\n",
      "# Example usage\n",
      "X = np.array([[1], [2], [3], [4], [5]])\n",
      "y = np.array([2, 4, 6, 8, 10])\n",
      "\n",
      "theta = linear_regression(X, y)\n",
      "print(\"Coefficients:\", theta)\n",
      "```\n",
      "\n",
      "This function takes in a matrix `X` of features and a vector `y` of target values, and calculates the coefficients of the linear regression model using the normal equation. The function then returns the coefficients as a vector `theta`. \n",
      "\n",
      "You can use this function with your own data by passing in the feature matrix `X` and target vector `y`.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mSure! Let's go through this script step by step to make sure we understand each part thoroughly.\n",
      "\n",
      "## Linear Regression Function\n",
      "\n",
      "The linear regression function defined here aims to calculate the optimal model parameters (coefficients) using the normal equation, a method for finding the best fit line for a given dataset in linear regression.\n",
      "\n",
      "Here is the function with elaborative details:\n",
      "\n",
      "### Imports\n",
      "```python\n",
      "import numpy as np\n",
      "```\n",
      "- **`import numpy as np`**: We import the NumPy library, referred to as `np`, which provides support for large, multi-dimensional arrays and matrices, and includes a number of mathematical functions to operate on these.\n",
      "\n",
      "### Function Definition\n",
      "```python\n",
      "def linear_regression(X, y):\n",
      "```\n",
      "- **`def linear_regression(X, y):`**: This defines a function `linear_regression` that takes in two arguments:\n",
      "  - `X`: The matrix of input features.\n",
      "  - `y`: The vector of target values.\n",
      "\n",
      "#### Adding the Intercept Term\n",
      "```python\n",
      "X = np.c_[np.ones(X.shape[0]), X]\n",
      "```\n",
      "- **`np.c_[np.ones(X.shape[0]), X]`**: NumPy's `c_` is used to concatenate arrays along the second axis (horizontally). `np.ones(X.shape[0])` creates an array of ones with length equal to the number of rows in `X`, which functions as the intercept term in linear regression (often needed for the bias term, so the model can fit an intercept).\n",
      "- **Purpose**: This effectively adds a column of ones to `X`, making the first column all ones, which helps include the intercept term in the regression calculation automatically.\n",
      "\n",
      "#### Calculating Coefficients\n",
      "```python\n",
      "theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
      "```\n",
      "- **`X.T`**: This is the transpose of `X`. In matrix operations for linear regression, you frequently deal with the transposed version of your feature matrix.\n",
      "- **`X.T.dot(X)`** and **`np.linalg.inv(X.T.dot(X))`**:\n",
      "  - `X.T.dot(X)`: Calculates the dot product of `X` transposed and `X`, which can be written as \\( X^T X \\).\n",
      "  - `np.linalg.inv(...)`: Takes the inverse of \\( X^T X \\). This computation is crucial and heavily depends on `X` having full rank (i.e., no singularity).\n",
      "- **`np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)`**:\n",
      "  - Once we have the inverse, we multiply it by `X.T` and then by `y`. The formula resembles \\( \\theta = (X^T X)^{-1} X^T y \\). This normal equation is key in computing the optimal parameters `theta` for linear regression.\n",
      "  \n",
      "### Returning Result\n",
      "```python\n",
      "return theta\n",
      "```\n",
      "- **`return theta`**: The function returns the coefficient vector `theta`.\n",
      "\n",
      "## Example Usage\n",
      "\n",
      "Now, let's see how the function is used with example data:\n",
      "\n",
      "### Sample Data\n",
      "```python\n",
      "X = np.array([[1], [2], [3], [4], [5]])\n",
      "y = np.array([2, 4, 6, 8, 10])\n",
      "```\n",
      "- **`X = np.array([[1], [2], [3], [4], [5]])`**: Creates a 2D NumPy array with features (independent variables). Here, there's only one feature, `X`.\n",
      "- **`y = np.array([2, 4, 6, 8, 10])`**: Creates a NumPy array for the dependent values. `y` is the vector of target values corresponding to `X`.\n",
      "\n",
      "### Running the Function\n",
      "```python\n",
      "theta = linear_regression(X, y)\n",
      "print(\"Coefficients:\", theta)\n",
      "```\n",
      "- **`linear_regression(X, y)`**: Calls the `linear_regression` function with `X` and `y` as arguments. It computes the regression coefficients and stores them in `theta`.\n",
      "- **`print(\"Coefficients:\", theta)`**: Prints the computed coefficients.\n",
      "\n",
      "In this particular example, the given `X` (features) and `y` (targets) make it clear that there’s a linear relationship \\( y = 2x \\). Hence, running the example results in coefficients close to `[0, 2]` read as `[Intercept, Slope]`.\n",
      "\n",
      "## Summary\n",
      "\n",
      "- The code is a straightforward implementation of linear regression using NumPy to demonstrate using the normal equation to find linear regression parameters.\n",
      "- Essential steps involve data preparation (adding intercept/bias term), applying matrix operations involving dot products, matrix transpose, and taking the inverse to solve for the coefficient vector `theta`.\n",
      "\n",
      "It serves as an educational piece to solidify understanding of linear regression through matrix algebra. Use this function with suitably structured dataset `X` (ensuring it’s not over-determined/singular) to garner useful predictions based on linear relationships.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'linear regression', 'output': 'Sure! Let\\'s go through this script step by step to make sure we understand each part thoroughly.\\n\\n## Linear Regression Function\\n\\nThe linear regression function defined here aims to calculate the optimal model parameters (coefficients) using the normal equation, a method for finding the best fit line for a given dataset in linear regression.\\n\\nHere is the function with elaborative details:\\n\\n### Imports\\n```python\\nimport numpy as np\\n```\\n- **`import numpy as np`**: We import the NumPy library, referred to as `np`, which provides support for large, multi-dimensional arrays and matrices, and includes a number of mathematical functions to operate on these.\\n\\n### Function Definition\\n```python\\ndef linear_regression(X, y):\\n```\\n- **`def linear_regression(X, y):`**: This defines a function `linear_regression` that takes in two arguments:\\n  - `X`: The matrix of input features.\\n  - `y`: The vector of target values.\\n\\n#### Adding the Intercept Term\\n```python\\nX = np.c_[np.ones(X.shape[0]), X]\\n```\\n- **`np.c_[np.ones(X.shape[0]), X]`**: NumPy\\'s `c_` is used to concatenate arrays along the second axis (horizontally). `np.ones(X.shape[0])` creates an array of ones with length equal to the number of rows in `X`, which functions as the intercept term in linear regression (often needed for the bias term, so the model can fit an intercept).\\n- **Purpose**: This effectively adds a column of ones to `X`, making the first column all ones, which helps include the intercept term in the regression calculation automatically.\\n\\n#### Calculating Coefficients\\n```python\\ntheta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\\n```\\n- **`X.T`**: This is the transpose of `X`. In matrix operations for linear regression, you frequently deal with the transposed version of your feature matrix.\\n- **`X.T.dot(X)`** and **`np.linalg.inv(X.T.dot(X))`**:\\n  - `X.T.dot(X)`: Calculates the dot product of `X` transposed and `X`, which can be written as \\\\( X^T X \\\\).\\n  - `np.linalg.inv(...)`: Takes the inverse of \\\\( X^T X \\\\). This computation is crucial and heavily depends on `X` having full rank (i.e., no singularity).\\n- **`np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)`**:\\n  - Once we have the inverse, we multiply it by `X.T` and then by `y`. The formula resembles \\\\( \\\\theta = (X^T X)^{-1} X^T y \\\\). This normal equation is key in computing the optimal parameters `theta` for linear regression.\\n  \\n### Returning Result\\n```python\\nreturn theta\\n```\\n- **`return theta`**: The function returns the coefficient vector `theta`.\\n\\n## Example Usage\\n\\nNow, let\\'s see how the function is used with example data:\\n\\n### Sample Data\\n```python\\nX = np.array([[1], [2], [3], [4], [5]])\\ny = np.array([2, 4, 6, 8, 10])\\n```\\n- **`X = np.array([[1], [2], [3], [4], [5]])`**: Creates a 2D NumPy array with features (independent variables). Here, there\\'s only one feature, `X`.\\n- **`y = np.array([2, 4, 6, 8, 10])`**: Creates a NumPy array for the dependent values. `y` is the vector of target values corresponding to `X`.\\n\\n### Running the Function\\n```python\\ntheta = linear_regression(X, y)\\nprint(\"Coefficients:\", theta)\\n```\\n- **`linear_regression(X, y)`**: Calls the `linear_regression` function with `X` and `y` as arguments. It computes the regression coefficients and stores them in `theta`.\\n- **`print(\"Coefficients:\", theta)`**: Prints the computed coefficients.\\n\\nIn this particular example, the given `X` (features) and `y` (targets) make it clear that there’s a linear relationship \\\\( y = 2x \\\\). Hence, running the example results in coefficients close to `[0, 2]` read as `[Intercept, Slope]`.\\n\\n## Summary\\n\\n- The code is a straightforward implementation of linear regression using NumPy to demonstrate using the normal equation to find linear regression parameters.\\n- Essential steps involve data preparation (adding intercept/bias term), applying matrix operations involving dot products, matrix transpose, and taking the inverse to solve for the coefficient vector `theta`.\\n\\nIt serves as an educational piece to solidify understanding of linear regression through matrix algebra. Use this function with suitably structured dataset `X` (ensuring it’s not over-determined/singular) to garner useful predictions based on linear relationships.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced scientst and Python programmer. Write a function tha implements concept of {concept}'\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "llm2 = ChatOpenAI(model_name='gpt-4o', temperature=1.2)\n",
    "\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the Python function {function}, decribe it as detailed as possible.'\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "output = overall_chain.invoke('linear regression')\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Let's go through this script step by step to make sure we understand each part thoroughly.\n",
      "\n",
      "## Linear Regression Function\n",
      "\n",
      "The linear regression function defined here aims to calculate the optimal model parameters (coefficients) using the normal equation, a method for finding the best fit line for a given dataset in linear regression.\n",
      "\n",
      "Here is the function with elaborative details:\n",
      "\n",
      "### Imports\n",
      "```python\n",
      "import numpy as np\n",
      "```\n",
      "- **`import numpy as np`**: We import the NumPy library, referred to as `np`, which provides support for large, multi-dimensional arrays and matrices, and includes a number of mathematical functions to operate on these.\n",
      "\n",
      "### Function Definition\n",
      "```python\n",
      "def linear_regression(X, y):\n",
      "```\n",
      "- **`def linear_regression(X, y):`**: This defines a function `linear_regression` that takes in two arguments:\n",
      "  - `X`: The matrix of input features.\n",
      "  - `y`: The vector of target values.\n",
      "\n",
      "#### Adding the Intercept Term\n",
      "```python\n",
      "X = np.c_[np.ones(X.shape[0]), X]\n",
      "```\n",
      "- **`np.c_[np.ones(X.shape[0]), X]`**: NumPy's `c_` is used to concatenate arrays along the second axis (horizontally). `np.ones(X.shape[0])` creates an array of ones with length equal to the number of rows in `X`, which functions as the intercept term in linear regression (often needed for the bias term, so the model can fit an intercept).\n",
      "- **Purpose**: This effectively adds a column of ones to `X`, making the first column all ones, which helps include the intercept term in the regression calculation automatically.\n",
      "\n",
      "#### Calculating Coefficients\n",
      "```python\n",
      "theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
      "```\n",
      "- **`X.T`**: This is the transpose of `X`. In matrix operations for linear regression, you frequently deal with the transposed version of your feature matrix.\n",
      "- **`X.T.dot(X)`** and **`np.linalg.inv(X.T.dot(X))`**:\n",
      "  - `X.T.dot(X)`: Calculates the dot product of `X` transposed and `X`, which can be written as \\( X^T X \\).\n",
      "  - `np.linalg.inv(...)`: Takes the inverse of \\( X^T X \\). This computation is crucial and heavily depends on `X` having full rank (i.e., no singularity).\n",
      "- **`np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)`**:\n",
      "  - Once we have the inverse, we multiply it by `X.T` and then by `y`. The formula resembles \\( \\theta = (X^T X)^{-1} X^T y \\). This normal equation is key in computing the optimal parameters `theta` for linear regression.\n",
      "  \n",
      "### Returning Result\n",
      "```python\n",
      "return theta\n",
      "```\n",
      "- **`return theta`**: The function returns the coefficient vector `theta`.\n",
      "\n",
      "## Example Usage\n",
      "\n",
      "Now, let's see how the function is used with example data:\n",
      "\n",
      "### Sample Data\n",
      "```python\n",
      "X = np.array([[1], [2], [3], [4], [5]])\n",
      "y = np.array([2, 4, 6, 8, 10])\n",
      "```\n",
      "- **`X = np.array([[1], [2], [3], [4], [5]])`**: Creates a 2D NumPy array with features (independent variables). Here, there's only one feature, `X`.\n",
      "- **`y = np.array([2, 4, 6, 8, 10])`**: Creates a NumPy array for the dependent values. `y` is the vector of target values corresponding to `X`.\n",
      "\n",
      "### Running the Function\n",
      "```python\n",
      "theta = linear_regression(X, y)\n",
      "print(\"Coefficients:\", theta)\n",
      "```\n",
      "- **`linear_regression(X, y)`**: Calls the `linear_regression` function with `X` and `y` as arguments. It computes the regression coefficients and stores them in `theta`.\n",
      "- **`print(\"Coefficients:\", theta)`**: Prints the computed coefficients.\n",
      "\n",
      "In this particular example, the given `X` (features) and `y` (targets) make it clear that there’s a linear relationship \\( y = 2x \\). Hence, running the example results in coefficients close to `[0, 2]` read as `[Intercept, Slope]`.\n",
      "\n",
      "## Summary\n",
      "\n",
      "- The code is a straightforward implementation of linear regression using NumPy to demonstrate using the normal equation to find linear regression parameters.\n",
      "- Essential steps involve data preparation (adding intercept/bias term), applying matrix operations involving dot products, matrix transpose, and taking the inverse to solve for the coefficient vector `theta`.\n",
      "\n",
      "It serves as an educational piece to solidify understanding of linear regression through matrix algebra. Use this function with suitably structured dataset `X` (ensuring it’s not over-determined/singular) to garner useful predictions based on linear relationships.\n"
     ]
    }
   ],
   "source": [
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_chain_studies-WWOOpBwl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
