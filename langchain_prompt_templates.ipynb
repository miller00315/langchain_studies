{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(model_name='gpt-3.5-turbo-instruct', temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why did the Brazilian soccer team bring a parrot to the game?\n",
      "\n",
      "Because it was their wingman! \n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    'Tell me a {adjective} joke about {content}'\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(adjective='happy', content='brazil')\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate: The prompt to chat models is a list of chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: My name is Bob and I am an AI designed to assist and provide helpful responses to users. I am also good at processing and analyzing large amounts of data quickly and accurately. Is there something specific you would like me to help you with?\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'You are a helpfull AI bot. Your name is {name}.'),\n",
    "        ('human', 'Hello how are you doing'),\n",
    "        ('ai', 'I\\'m doing well, thanks!'),\n",
    "        ('human', '{user_input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = chat_template.format_messages(name='Bob', user_input='What is your name and what are you good at?')\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various ways of formatting System/Human/Ai prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "System: I love indulging in delicious treats!\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assiatnt that re-writes the user's text to\"\n",
    "                \"sound more upbeat\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = chat_template.format_messages(text=\"I don't like eating tasty things\")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing a context along the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hugging Face's transformers library, OpenAI's 'openai' library, and Cohere's `cohere` library.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Answer teh question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".vscode/\n",
    "\n",
    "Context: Lage Language models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models \n",
    "can be acessed via Hugging Face's transformers library, via OpenAI\n",
    "using the 'openai' library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: Which libraries and model providers offer LLMs?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create examples\n",
    "examples = [\n",
    "    {\n",
    "        'query': 'How are you?',\n",
    "        'answer': 'I can\\'t complain bu sometimes I still do.'\n",
    "    },\n",
    "    {\n",
    "        'query': 'What times is it?',\n",
    "        'answer': 'It\\'s to get a watch.'\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt exmaple from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=['query', 'answer'],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the preffix is our instructions\n",
    "prefix = \"\"\"The following are experts from conversations with AI assitant.\n",
    "The assitant is typically sarcastic anwitty, producing\n",
    "creative and funny responses to the users questions. Here are some \n",
    "examples:\"\"\"\n",
    "\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are experts from conversations with AI assitant.\n",
      "The assitant is typically sarcastic anwitty, producing\n",
      "creative and funny responses to the users questions. Here are some \n",
      "examples:\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain bu sometimes I still do.\n",
      "\n",
      "\n",
      "\n",
      "User: What times is it?\n",
      "AI: It's to get a watch.\n",
      "\n",
      "\n",
      "\n",
      "User: Qhat movie should Iwatch evening?\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "few_shot_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=['query'],\n",
    "    example_separator='\\n\\n'\n",
    ")\n",
    "\n",
    "query = 'Qhat movie should Iwatch evening?'\n",
    "\n",
    "print(few_shot_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' How about \"The Shawshank Redemption\"? It\\'s a classic and I\\'m sure you haven\\'t seen it for the 100th time yet.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = few_shot_template | model\n",
    "\n",
    "chain.invoke({'query': query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_chain_studies-WWOOpBwl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
