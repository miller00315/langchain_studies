{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miller/.local/share/virtualenvs/lang_chain_studies-WWOOpBwl/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "model = OpenAI(model_name='gpt-3.5-turbo-instruct', temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why did the Portuguese man go to the doctor?\n",
      "\n",
      "Because he was feeling a little Porto-sick!\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    'Tell me a {adjective} joke about {content}'\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(adjective='happy', content='Portugal')\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate: The prompt to chat models is a list of chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: My name is Bob and I am an AI designed to assist and provide helpful responses to users. I am also good at processing and analyzing large amounts of data quickly and accurately. Is there something specific you would like me to help you with?\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'You are a helpfull AI bot. Your name is {name}.'),\n",
    "        ('human', 'Hello how are you doing'),\n",
    "        ('ai', 'I\\'m doing well, thanks!'),\n",
    "        ('human', '{user_input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = chat_template.format_messages(name='Bob', user_input='What is your name and what are you good at?')\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various ways of formatting System/Human/Ai prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "System: I love indulging in delicious treats!\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assiatnt that re-writes the user's text to\"\n",
    "                \"sound more upbeat\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = chat_template.format_messages(text=\"I don't like eating tasty things\")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing a context along the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hugging Face's transformers library, OpenAI's 'openai' library, and Cohere's `cohere` library.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Answer teh question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".vscode/\n",
    "\n",
    "Context: Lage Language models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models \n",
    "can be acessed via Hugging Face's transformers library, via OpenAI\n",
    "using the 'openai' library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: Which libraries and model providers offer LLMs?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create examples\n",
    "examples = [\n",
    "    {\n",
    "        'query': 'How are you?',\n",
    "        'answer': 'I can\\'t complain bu sometimes I still do.'\n",
    "    },\n",
    "    {\n",
    "        'query': 'What times is it?',\n",
    "        'answer': 'It\\'s to get a watch.'\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt exmaple from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=['query', 'answer'],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the preffix is our instructions\n",
    "prefix = \"\"\"The following are experts from conversations with AI assitant.\n",
    "The assitant is typically sarcastic anwitty, producing\n",
    "creative and funny responses to the users questions. Here are some \n",
    "examples:\"\"\"\n",
    "\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are experts from conversations with AI assitant.\n",
      "The assitant is typically sarcastic anwitty, producing\n",
      "creative and funny responses to the users questions. Here are some \n",
      "examples:\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain bu sometimes I still do.\n",
      "\n",
      "\n",
      "\n",
      "User: What times is it?\n",
      "AI: It's to get a watch.\n",
      "\n",
      "\n",
      "\n",
      "User: Qhat movie should Iwatch evening?\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "few_shot_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=['query'],\n",
    "    example_separator='\\n\\n'\n",
    ")\n",
    "\n",
    "query = 'Qhat movie should Iwatch evening?'\n",
    "\n",
    "print(few_shot_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' How about \"The Shawshank Redemption\"? It\\'s a classic and I\\'m sure you haven\\'t seen it for the 100th time yet.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = few_shot_template | model\n",
    "\n",
    "chain.invoke({'query': query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tou are a experience virologist.\\nWrite a few sentences about the following virus hiv in german.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = '''You are a experience virologist.\n",
    "Write a few sentences about the following virus {virus} in {language}.'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(virus='hiv', language='german')\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='HIV, das humane Immundefizienzvirus, ist ein Virus, das das Immunsystem des Menschen schwächt und zu AIDS führen kann. Es wird hauptsächlich durch ungeschützten Geschlechtsverkehr, den Austausch von infizierten Nadeln oder von der Mutter auf das Kind während der Schwangerschaft übertragen. Es ist wichtig, sich über die Übertragungswege und Präventionsmaßnahmen von HIV zu informieren, um die Verbreitung dieser lebensbedrohlichen Krankheit einzudämmen.' response_metadata={'token_usage': {'completion_tokens': 126, 'prompt_tokens': 30, 'total_tokens': 156}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-69de31e6-404d-48ee-96e4-5b28ac7d5ab2-0' usage_metadata={'input_tokens': 30, 'output_tokens': 126, 'total_tokens': 156}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "output = llm.invoke(prompt)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You respond only in the json format.'),\n",
       " HumanMessage(content='Top 20 countries in Europe by population.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in the json format.'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(n='20', area='Europe')\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='{\\n    \"1\": \"Germany\",\\n    \"2\": \"Russia\",\\n    \"3\": \"Turkey\",\\n    \"4\": \"France\",\\n    \"5\": \"United Kingdom\",\\n    \"6\": \"Italy\",\\n    \"7\": \"Spain\",\\n    \"8\": \"Ukraine\",\\n    \"9\": \"Poland\",\\n    \"10\": \"Romania\",\\n    \"11\": \"Netherlands\",\\n    \"12\": \"Belgium\",\\n    \"13\": \"Czech Republic\",\\n    \"14\": \"Greece\",\\n    \"15\": \"Portugal\",\\n    \"16\": \"Sweden\",\\n    \"17\": \"Hungary\",\\n    \"18\": \"Belarus\",\\n    \"19\": \"Austria\",\\n    \"20\": \"Serbia\"\\n}' response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 28, 'total_tokens': 184}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-af2d831e-e21b-46b9-8bbe-d8b6b2b27a34-0' usage_metadata={'input_tokens': 28, 'output_tokens': 156, 'total_tokens': 184}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_chain_studies-WWOOpBwl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
